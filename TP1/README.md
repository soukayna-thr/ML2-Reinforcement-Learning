 # TP1 ‚Äì Initiation au Reinforcement Learning avec OpenAI Gym ü§ñ

## üìå Contexte et Objectif

Ce TP fait partie du module "Machine Learning II". L'objectif est de se familiariser avec les outils essentiels du Reinforcement Learning (RL) en utilisant principalement la biblioth√®que OpenAI Gym. Nous avons apprendre √† interagir avec un environnement RL, √† ex√©cuter des actions et √† comprendre les notions de base comme l'espace d'actions, les observations, les r√©compenses et le crit√®re de fin d'√©pisode.

## üöÄ Partie 1 : Pr√©sentation des Biblioth√®ques Cl√©s

### OpenAI Gym
![img](https://gymnasium.farama.org/_images/gymnasium-text.png)

- **Description :**
  - OpenAI Gym fournit un ensemble d'environnements interactifs qui permettent de tester et de comparer diff√©rents algorithmes d'apprentissage par renforcement.
  - Un environnement Gym est d√©fini par un ensemble **d'√©tats**, **d'actions**, **de r√©compenses**, et **un crit√®re de fin**.

- **Installation :**

  Pour installer Gym ainsi que quelques biblioth√®ques associ√©es (pygame, numpy, etc.), ex√©cutez la commande suivante :
  
  ```bash
  pip install --upgrade gymnasium pygame numpy
  
## üõ†Ô∏è Partie 2 : Exercices Pratiques avec OpenAI Gym

### Exercice 1 : D√©couverte et Exploration d'un Environnement
**Objectif :**  
Comprendre la structure d'un environnement Gym en examinant ses espaces d'actions et d'observations.

---

### Exercice 2 : Manipulation des Observations et R√©compenses
**Objectif :**  
Analyser comment r√©cup√©rer l'observation, la r√©compense et l'√©tat de fin (`done`) apr√®s l'ex√©cution d'une action.

---

### Exercice 3 : Contr√¥le Manuel de l'Agent
**Objectif :**  
Permettre √† l'utilisateur de contr√¥ler manuellement l'agent pour observer directement l'effet de ses actions.

---

### Exercice 4 : √âvaluation des Performances d'une Politique Al√©atoire
**Objectif :**  
Mesurer la dur√©e moyenne d'un √©pisode lorsque l'agent prend des actions al√©atoires.


## üéØ  Informations : CartPole - OpenAI Gym


### Objectif
L'agent doit apprendre √† garder l'√©quilibre du poteau aussi longtemps que possible en s√©lectionnant les bonnes actions.
![Image1](https://www.gymlibrary.dev/_images/cart_pole.gif)

### Environnement

#### 1. **Espace d'√©tat (Observation Space)**
L'√©tat est repr√©sent√© par un vecteur de 4 valeurs continues :
- **Position du chariot** (x)
- **Vitesse du chariot** (\(\dot{x}\))
- **Angle du poteau** (\(\theta\))
- **Vitesse angulaire du poteau** (\(\dot{\theta}\))

#### 2. **Espace d'actions (Action Space)**
L'agent peut prendre deux actions discr√®tes :
- `0` : Appliquer une force vers la gauche
- `1` : Appliquer une force vers la droite

#### 3. **R√©compense (Reward)**
L'agent re√ßoit une **r√©compense de +1** pour chaque √©tape o√π le poteau reste en √©quilibre.

#### 4. **Conditions de fin**
L'√©pisode se termine si :
- L'angle du poteau d√©passe **¬±12¬∞**
- La position du chariot d√©passe **¬±2.4 unit√©s**
- L'√©pisode atteint **500 √©tapes**

Pour plus d'informations, consulter : üìö[Documentation CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) 
